---
title: "EXAM_SL"
author: "roi hezkiyahu - 205884018"
date: "2023-01-23"
output: html_document
---

### imports

```{r message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)
library(purrr)
library(caret)
library(class)
library(MASS)
library(tidyverse)
library(nnet)
library(keras)
```


# Q1

* loading the data

```{r}
df_train <- read.table("zip.train") %>% rename("y" = "V1") %>%
  filter(y %in% c(2,3,5))
df_test <- read.table("zip.test") %>% rename("y" = "V1") %>%
  filter(y %in% c(2,3,5))
X_train = df_train[,2:257]
y_train = df_train[,1]
X_test = df_test[,2:257]
y_test = df_test[,1]
```


### 1.a

* using PCA

```{r}
?prcomp
pca <- prcomp(X_train)
plot(cumsum(pca$sdev/sum(pca$sdev)),
     xlab = "PC",
     ylab = "Cumulative Proportion of Variance Explained")
```

* i will take 100 PCS which explaines ~ 75% of the variance in the data.


```{r}
X_pca_train_100 = as_tibble(pca$x[,1:100])
X_pca_test_100 = as_tibble(predict(pca,X_test)[,1:100])
```


```{r}
lda_full = lda(y~., df_train)
df_pca_train_100 = X_pca_train_100 %>% mutate(y=y_train)
lda_pca = lda(y~., df_pca_train_100)
```


```{r}
get_results = function(model, X, y_true, lda=TRUE, neural_model=FALSE){
  # this function returns the confusion matrix and miss classification error of a given model
  y_pred = predict(model, X)
  if (lda) {
     y_pred = y_pred$class 
  }
  if (neural_model){
    y_pred = apply(y_pred,1,which.max)
    y_pred[y_pred==1] = 5
  }
  classification_err = mean(y_pred != y_true)
  conf_matrix = confusionMatrix(as.factor(y_true), as.factor(y_pred))
  return( list("classification_err" = classification_err,
               "conf_matrix" =   conf_matrix$table))
}
lda_train_results = get_results(lda_full, X_train, y_train, TRUE)
lda_test_results = get_results(lda_full, X_test, y_test, TRUE)
lda_pca_train_results = get_results(lda_pca, X_pca_train_100, y_train, TRUE)
lda_pca_test_results = get_results(lda_pca, X_pca_test_100, y_test, TRUE)
tibble(model_dataset = c("lda train", "lda test", "lda with pca train", "lda with pca test"),
       err = c(lda_train_results$classification_err, lda_test_results$classification_err, lda_pca_train_results$classification_err,  lda_pca_test_results$classification_err))
```


* we can see that both the model with the PCA and the model without the PCA preform rather similar on the test set (9.5% error) but the model on the full data tends to overfits more then the PCA. meaning the PCA here helps reduce the overfit.

* lets take a look at the confution matrix of the full LDA

* train:

```{r echo=FALSE}
lda_train_results$conf_matrix
```

we can see that the model struggels more to distinguish between 5 and 3, after that it has a hard time with 3 and 2, and can seperate well enougth 5 and 2

* test:

```{r echo=FALSE}
lda_test_results$conf_matrix
```
* we can see in general the same behavior as in the train set, but with larger percentage of mistake, notice that the test set is smaller then the train set and that we have higher values outside the diagonal (which indicate false prediction)


### 1.b

```{r}
multinom_full = multinom(y~., df_train)
multinom_pca = multinom(y~., df_pca_train_100)
```


```{r}
multinom_train_results = get_results(multinom_full, X_train, y_train, FALSE)
multinom_test_results = get_results(multinom_full, X_test, y_test, FALSE)
multinom_pca_train_results = get_results(multinom_pca, X_pca_train_100, y_train, FALSE)
multinom_pca_test_results = get_results(multinom_pca, X_pca_test_100, y_test, FALSE)
tibble(model_dataset = c("multinom train", "multinom test", "multinom with pca train", "multinom with pca test"),
       err = c(multinom_train_results$classification_err, multinom_test_results$classification_err, multinom_pca_train_results$classification_err,  multinom_pca_test_results$classification_err))
```


* clearly we overfit the data, we can see a train error of 0, which means that in the train set we are able to classify each observation to the correct y value, but for the test set we have much larger errors 14.5% for the full multi-class logistic regression, and 9.35% for the multi-class logistic regression with PCA.
* we see that in this case the PCA helps improve the results of the model, also the multi-class logistic regression with PCA has similar results to the LDA


* lets take a look at the confution matrix of the PCA multi-class logistic regression
* the train has perfect score thus the confution matrix is diagonal and not that interesting, lets see the test confution matrix:


```{r echo=FALSE}
multinom_pca_test_results$conf_matrix
```

* we can see a rather similar performance, the model struggles the most with 5 and 3, after that with 3 and 2, and is rather ok with 5 and 2.


### 1.c

* we are using images, and as we know the SOTA for image predictive modeling are CNNS so lets make a CNN to predict the class 

* first lets prepare the data:
- the loss expects values to be between 0-2, so i will reencode 5 => 0, 2=> 1, 3 => 2

```{r}
X_train_net = array_reshape(as.matrix(X_train), c(nrow(X_train),16,16,1))
y_train_net = y_train
y_train_net[y_train_net == 5] = 1

X_test_net = array_reshape(as.matrix(X_test), c(nrow(X_test),16,16,1))
y_test_net = y_test
y_test_net[y_test_net == 5] = 1
```


* now create and train the model

```{r message=FALSE}
cnn_model <- keras_model_sequential() %>%
  layer_conv_2d(filters = 16, kernel_size = c(3, 3), activation = "relu", input_shape = c(16, 16, 1)) %>%
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_flatten() %>%
  layer_batch_normalization() %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 3, activation = "softmax")


summary(cnn_model)
```


```{r}
cnn_model %>% compile(loss = 'sparse_categorical_crossentropy',optimizer = optimizer_adam(learning_rate = 0.0001),metrics = c('accuracy'))
cnn_model %>% fit(X_train_net, y_train_net-1, batch_size = 64,epochs = 100,verbose = 1,
                  class_weight = list("0" = 1.5, "1" = 0.7, "2" = 1))
```



```{r}
cnn_train_res = get_results(cnn_model, X_train_net, y_train, FALSE, TRUE)
cnn_test_res = get_results(cnn_model, X_test_net, y_test, FALSE, TRUE)
tibble(model_dataset = c("neural net train", "neural net test"),
       err = c(cnn_train_res$classification_err, cnn_test_res$classification_err))
```

* we can see that this model is far better then the ones before, also it still has a much lower error in the train set, we can keep playing with the paramaters of the model (add some more regularization or use data augmentation -[which for some reason doesnt work for me in r]) to get even better results


* lets take a look at the test confution matrix


```{r echo=FALSE}
cnn_test_res$conf_matrix
```

* we can see that the results are much better, we still some dificulties with the 5's which is most chakkenging to classify.

* the over all results:

```{r echo=FALSE}
tibble(model_dataset = c("lda train", "lda test", "lda with pca train", "lda with pca test", "multinom train", "multinom test", "multinom with pca train", "multinom with pca test", "neural net train", "neural net test"),
       err = c(lda_train_results$classification_err, lda_test_results$classification_err, lda_pca_train_results$classification_err,  lda_pca_test_results$classification_err, multinom_train_results$classification_err, multinom_test_results$classification_err, multinom_pca_train_results$classification_err,  multinom_pca_test_results$classification_err, cnn_train_res$classification_err, cnn_test_res$classification_err)) %>% arrange(err)
```

* we can see from the full results that the multinom and neural net both have perfect score on the train set, also the neural net has the best results on the test set, and even better results on the rest set then other models get on the train set!


