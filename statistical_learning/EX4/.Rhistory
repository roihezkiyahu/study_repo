big_tree <- decision_tree(mode = "regression") %>%
set_engine("rpart")
big_forest <- rand_forest(mode = "regression", trees = 100) %>%
set_engine("randomForest")
small_forest <- rand_forest(mode = "regression", trees = 100, min_n = 50) %>%
set_engine("randomForest")
cv_v <- 5
cv_splits<- vfold_cv(train, v = cv_v)
cv_results <- tibble(
"1-SE" = map_dbl(cv_splits$splits,cv_model,tree_spec,TRUE),
"small_tree" = map_dbl(cv_splits$splits,cv_model,small_tree),
"big_tree" = map_dbl(cv_splits$splits,cv_model,big_tree),
"big_forest" = map_dbl(cv_splits$splits,cv_model,big_forest),
"small_forest" = map_dbl(cv_splits$splits,cv_model,small_forest))
plot_cv_res(cv_results)
########### Training data (rankings only, no dates):
set.seed(123)
con <- url("http://www.tau.ac.il/~saharon/StatsLearn2022/train_ratings_all.dat")
X.tr <- read.table(con)
con <- url("http://www.tau.ac.il/~saharon/StatsLearn2022/train_y_rating.dat")
y.tr <- read.table(con)
con = url("http://www.tau.ac.il/~saharon/StatsLearn2022/movie_titles.txt")
titles = read.table(con,sep=",")
names(X.tr) = substr(as.character(titles[,2]),1,15)
movies = substr(as.character(titles[,2]),1,15)
########### Divide training data into training and validation
n = dim(X.tr)[1]
nva=2000
va.id = sample (n,nva) # choose 2000 points for validation
trtr = data.frame (X = X.tr[-va.id,],y=(y.tr[-va.id,]>3) - (y.tr[-va.id,]<=3))
va = data.frame (X = X.tr[va.id,],y=(y.tr[va.id,]>3)- (y.tr[va.id,]<=3))
############# AdaBoost
train_val_ada_boost <- function(maxdepth = 2, cp = 0.00001, epsilon = 0, n_iter = 1000, exp_loss= TRUE){
train_miss_calssification <- c()
test_miss_calssification <- c()
w.now = rep (1, dim(trtr)[1]) # initialize w=1
err.boost=err.tr.boost=NULL
pred.boost = numeric(dim(va)[1])
tr.boost = numeric(dim(trtr)[1])
for (i in 1:n_iter){
tree.mod= rpart (y~.,data=trtr,method="class",weights=w.now,maxdepth=maxdepth,cp=cp)
yhat.now = predict(tree.mod,type="class")
yhat.now.num = as.numeric(as.character(yhat.now))
if (exp_loss){
Err = sum( w.now*(yhat.now != trtr$y))/sum(w.now)
}
else {
Err = sum( w.now*((as.numeric(yhat.now) - trtr$y)^2))/sum(w.now)
}
if (epsilon == 0){
alpha = 0.5*log((1-Err)/Err)
}
else {alpha = epsilon}
# for sanity check:
# cat(i,Err, alpha,"\n")
pred.boost = pred.boost + alpha*as.numeric(as.character(predict(tree.mod, newdata=va,type="class")))
tr.boost = tr.boost + alpha*yhat.now.num
w.now = w.now * exp(-alpha*yhat.now.num*trtr$y)
w.now = w.now/sum(w.now)*dim(trtr)[1] # rescaling - not important!
if (exp_loss){
train_err = mean (sign(tr.boost)!=trtr$y)
test_err = mean (sign(pred.boost)!=va$y)
}
else{
train_err = mean((tr.boost - trtr$y)^2)
test_err = mean((pred.boost - va$y)^2)
}
# cat (i, "train:", train_err, " test:", test_err,"\n")
train_miss_calssification = c(train_miss_calssification,train_err)
test_miss_calssification = c(test_miss_calssification,test_err)
}
out_tbl <- tibble(train_err = train_miss_calssification,
test_err = test_miss_calssification,
iteration = seq(1,n_iter))
return(out_tbl)
}
res1 = train_val_ada_boost()
res1 %>%
pivot_longer(cols = !iteration,names_to = "err_type",values_to = "err") %>%
ggplot(aes(iteration, err,group=err_type,color = err_type)) +
geom_line(aes(group=err_type)) +
geom_point() +
theme_light()
res2 <- train_val_ada_boost(epsilon = 0.01)
res2 %>%
pivot_longer(cols = !iteration,names_to = "err_type",values_to = "err") %>%
ggplot(aes(iteration, err,group=err_type,color = err_type)) +
geom_line(aes(group=err_type)) +
geom_point() +
theme_light()
# train_val_ada_boost(exp_loss = FALSE, n_iter = 5)
res4 <- train_val_ada_boost(cp = 0, maxdepth = 3)
res4 %>%
pivot_longer(cols = !iteration,names_to = "err_type",values_to = "err") %>%
ggplot(aes(iteration, err,group=err_type,color = err_type)) +
geom_line(aes(group=err_type)) +
geom_point() +
theme_light()
res5 <- train_val_ada_boost(maxdepth = 1, n_iter = 1000)
res5 %>%
pivot_longer(cols = !iteration,names_to = "err_type",values_to = "err") %>%
ggplot(aes(iteration, err,group=err_type,color = err_type)) +
geom_line(aes(group=err_type)) +
geom_point() +
theme_light()
res2 <- train_val_ada_boost(epsilon = 0.001, n_iter = 2500)
res2 %>%
pivot_longer(cols = !iteration,names_to = "err_type",values_to = "err") %>%
ggplot(aes(iteration, err,group=err_type,color = err_type)) +
geom_line(aes(group=err_type)) +
geom_point() +
theme_light()
res2 <- train_val_ada_boost(epsilon = 0.005, n_iter = 2500)
res2 %>%
pivot_longer(cols = !iteration,names_to = "err_type",values_to = "err") %>%
ggplot(aes(iteration, err,group=err_type,color = err_type)) +
geom_line(aes(group=err_type)) +
geom_point() +
theme_light()
res2 <- train_val_ada_boost(epsilon = 0.01, n_iter = 2500, cp = 0.0001)
res2 %>%
pivot_longer(cols = !iteration,names_to = "err_type",values_to = "err") %>%
ggplot(aes(iteration, err,group=err_type,color = err_type)) +
geom_line(aes(group=err_type)) +
geom_point() +
theme_light()
res2 <- train_val_ada_boost(epsilon = 0.001, n_iter = 2500, cp = 0.0001)
res2 %>%
pivot_longer(cols = !iteration,names_to = "err_type",values_to = "err") %>%
ggplot(aes(iteration, err,group=err_type,color = err_type)) +
geom_line(aes(group=err_type)) +
geom_point() +
theme_light()
res2 <- train_val_ada_boost(epsilon = 0.005, n_iter = 2500, cp = 0.0001)
res2 %>%
pivot_longer(cols = !iteration,names_to = "err_type",values_to = "err") %>%
ggplot(aes(iteration, err,group=err_type,color = err_type)) +
geom_line(aes(group=err_type)) +
geom_point() +
theme_light()
res5 <- train_val_ada_boost(maxdepth = 1, n_iter = 2500)
res5 %>%
pivot_longer(cols = !iteration,names_to = "err_type",values_to = "err") %>%
ggplot(aes(iteration, err,group=err_type,color = err_type)) +
geom_line(aes(group=err_type)) +
geom_point() +
theme_light()
res5 <- train_val_ada_boost(maxdepth = 1, n_iter = 2500, cp = 0.0001)
res5 %>%
pivot_longer(cols = !iteration,names_to = "err_type",values_to = "err") %>%
ggplot(aes(iteration, err,group=err_type,color = err_type)) +
geom_line(aes(group=err_type)) +
geom_point() +
theme_light()
res5 <- train_val_ada_boost(maxdepth = 1, n_iter = 2500, cp = 0.000001)
res5 %>%
pivot_longer(cols = !iteration,names_to = "err_type",values_to = "err") %>%
ggplot(aes(iteration, err,group=err_type,color = err_type)) +
geom_line(aes(group=err_type)) +
geom_point() +
theme_light()
res2 <- train_val_ada_boost(epsilon = 0.0025, n_iter = 2500)
res2 %>%
pivot_longer(cols = !iteration,names_to = "err_type",values_to = "err") %>%
ggplot(aes(iteration, err,group=err_type,color = err_type)) +
geom_line(aes(group=err_type)) +
geom_point() +
theme_light()
res2 <- train_val_ada_boost(epsilon = 0.0075, n_iter = 2500)
res2 %>%
pivot_longer(cols = !iteration,names_to = "err_type",values_to = "err") %>%
ggplot(aes(iteration, err,group=err_type,color = err_type)) +
geom_line(aes(group=err_type)) +
geom_point() +
theme_light()
res2 <- train_val_ada_boost(epsilon = 0.005, n_iter = 2500, cp = 0.0001)
res2 %>%
pivot_longer(cols = !iteration,names_to = "err_type",values_to = "err") %>%
ggplot(aes(iteration, err,group=err_type,color = err_type)) +
geom_line(aes(group=err_type)) +
geom_point() +
theme_light()
res2 <- train_val_ada_boost(epsilon = 0.005, n_iter = 2500, cp = 0.0005)
res2 %>%
pivot_longer(cols = !iteration,names_to = "err_type",values_to = "err") %>%
ggplot(aes(iteration, err,group=err_type,color = err_type)) +
geom_line(aes(group=err_type)) +
geom_point() +
theme_light()
res2 <- train_val_ada_boost(epsilon = 0.005, n_iter = 2500, cp = 0.001)
res2 %>%
pivot_longer(cols = !iteration,names_to = "err_type",values_to = "err") %>%
ggplot(aes(iteration, err,group=err_type,color = err_type)) +
geom_line(aes(group=err_type)) +
geom_point() +
theme_light()
res2 <- train_val_ada_boost(epsilon = 0.005, n_iter = 2500, cp = 0.00001)
res2 %>%
pivot_longer(cols = !iteration,names_to = "err_type",values_to = "err") %>%
ggplot(aes(iteration, err,group=err_type,color = err_type)) +
geom_line(aes(group=err_type)) +
geom_point() +
theme_light()
res2 <- train_val_ada_boost(epsilon = 0.005, n_iter = 2500, cp = 0.0005)
keras::install_keras() # run this only the first time
# need to install package keras, and also have python on the computer (Anaconda 3.#)
library(keras)
install.packages(keras)
# need to install package keras, and also have python on the computer (Anaconda 3.#)
library(keras)
library(reticulate)
install.packages("installr") library(installr) updateR()
install.packages("installr") library(installr) updateR()
install.packages("installr")
library(installr)
updateR()
library(dplyr)
library(ggplot2)
library(purrr)
library(caret)
library(class)
library(tidymodels)
library(furrr)
library(rpart)
library(rpart)
library(keras)
plot_cv_res <- function(cv_results){
cv_results_longer <- cv_results %>%
pivot_longer(cols = colnames(cv_results),names_to = "recipe",values_to = "rmse") %>% mutate(fold = ceiling(1:(length(colnames(cv_results))*cv_v)/length(colnames(cv_results))))
cv_results_longer %>% group_by(recipe) %>% summarize(m = median(rmse)) %>% arrange(m)
cv_results_longer %>%
mutate(fold = factor(fold)) %>%
ggplot(aes(recipe, rmse,group=fold,color = fold)) +
geom_line(aes(group=fold)) +
geom_point() +
theme_light()
}
set.seed(123)
con <- url("http://www.tau.ac.il/~saharon/StatsLearn2022/train_ratings_all.dat")
X <- tibble(read.table(con))
con <- url("http://www.tau.ac.il/~saharon/StatsLearn2022/train_y_rating.dat")
y <- read.table(con)
cv_model <- function(df_split, model, prune_tree = FALSE){
train_df <- training(df_split)
test_df <- testing(df_split)
if (prune_tree){
tree.mod= rpart (y~.,data=train_df,cp=0.0001)
tree.res = printcp(tree.mod)
#1-SE rule
chosen.prune = min ((1:dim(tree.res)[1]) [tree.res[,"xerror"] < min(tree.res[,"xerror"]+tree.res[,"xstd"])])
model = prune(tree.mod, cp=tree.res[chosen.prune,"CP"])
model_pred <- predict(model, test_df %>% select(-y))
rmse_vec(test_df$y, model_pred)
}
else{
model <- model %>% fit(formula = y~., data = train_df)
model_pred <- predict(model, test_df %>% select(-y))
rmse_vec(test_df$y, model_pred$.pred)
}
}
train <- X %>% mutate(y = y %>% pull)
small_tree <- decision_tree(mode = "regression", tree_depth = 5) %>%
set_engine("rpart")
big_tree <- decision_tree(mode = "regression") %>%
set_engine("rpart")
big_forest <- rand_forest(mode = "regression", trees = 100) %>%
set_engine("randomForest")
small_forest <- rand_forest(mode = "regression", trees = 100, min_n = 50) %>%
set_engine("randomForest")
cv_v <- 5
cv_splits<- vfold_cv(train, v = cv_v)
cv_results <- tibble(
"1-SE" = map_dbl(cv_splits$splits,cv_model,tree_spec,TRUE),
"small_tree" = map_dbl(cv_splits$splits,cv_model,small_tree),
"big_tree" = map_dbl(cv_splits$splits,cv_model,big_tree),
"big_forest" = map_dbl(cv_splits$splits,cv_model,big_forest),
"small_forest" = map_dbl(cv_splits$splits,cv_model,small_forest))
plot_cv_res(cv_results)
########### Training data (rankings only, no dates):
set.seed(123)
con <- url("http://www.tau.ac.il/~saharon/StatsLearn2022/train_ratings_all.dat")
X.tr <- read.table(con)
con <- url("http://www.tau.ac.il/~saharon/StatsLearn2022/train_y_rating.dat")
y.tr <- read.table(con)
con = url("http://www.tau.ac.il/~saharon/StatsLearn2022/movie_titles.txt")
titles = read.table(con,sep=",")
names(X.tr) = substr(as.character(titles[,2]),1,15)
movies = substr(as.character(titles[,2]),1,15)
########### Divide training data into training and validation
n = dim(X.tr)[1]
nva=2000
va.id = sample (n,nva) # choose 2000 points for validation
trtr = data.frame (X = X.tr[-va.id,],y=(y.tr[-va.id,]>3) - (y.tr[-va.id,]<=3))
va = data.frame (X = X.tr[va.id,],y=(y.tr[va.id,]>3)- (y.tr[va.id,]<=3))
############# AdaBoost
train_val_ada_boost <- function(maxdepth = 2, cp = 0.00001, epsilon = 0, n_iter = 1000){
train_miss_calssification <- c()
test_miss_calssification <- c()
w.now = rep (1, dim(trtr)[1]) # initialize w=1
err.boost=err.tr.boost=NULL
pred.boost = numeric(dim(va)[1])
tr.boost = numeric(dim(trtr)[1])
for (i in 1:n_iter){
tree.mod= rpart (y~.,data=trtr,method="class",weights=w.now,maxdepth=maxdepth,cp=cp)
yhat.now = predict(tree.mod,type="class")
yhat.now.num = as.numeric(as.character(yhat.now))
Err = sum( w.now*(yhat.now != trtr$y))/sum(w.now)
if (epsilon == 0){
alpha = 0.5*log((1-Err)/Err)
}
else {alpha = epsilon}
# for sanity check:
# cat(i,Err, alpha,"\n")
pred.boost = pred.boost + alpha*as.numeric(as.character(predict(tree.mod, newdata=va,type="class")))
tr.boost = tr.boost + alpha*yhat.now.num
w.now = w.now * exp(-alpha*yhat.now.num*trtr$y)
w.now = w.now/sum(w.now)*dim(trtr)[1] # rescaling - not important!
train_err = mean (sign(tr.boost)!=trtr$y)
test_err = mean (sign(pred.boost)!=va$y)
# cat (i, "train:", train_err, " test:", test_err,"\n")
train_miss_calssification = c(train_miss_calssification,train_err)
test_miss_calssification = c(test_miss_calssification,test_err)
}
out_tbl <- tibble(train_err = train_miss_calssification,
test_err = test_miss_calssification,
iteration = seq(1,n_iter))
return(out_tbl)
}
res1 = train_val_ada_boost()
res1 %>%
pivot_longer(cols = !iteration,names_to = "err_type",values_to = "err") %>%
ggplot(aes(iteration, err,group=err_type,color = err_type)) +
geom_line(aes(group=err_type)) +
geom_point() +
theme_light()
res2 <- train_val_ada_boost(epsilon = 0.01)
res2 %>%
pivot_longer(cols = !iteration,names_to = "err_type",values_to = "err") %>%
ggplot(aes(iteration, err,group=err_type,color = err_type)) +
geom_line(aes(group=err_type)) +
geom_point() +
theme_light()
train_val_ada_boost_mse <- function(maxdepth = 2, cp = 0.00001, epsilon = 0.01, n_iter = 1000){
gbm_tr = trtr
train_miss_calssification <- c()
test_miss_calssification <- c()
w.now = rep (1, dim(trtr)[1]) # initialize w=1
err.boost=err.tr.boost=NULL
pred.boost = numeric(dim(va)[1])
tr.boost = numeric(dim(trtr)[1])
alpha = epsilon
for (i in 1:n_iter){
tree.mod= rpart (y~.,data=gbm_tr,method="anova",maxdepth=maxdepth,cp=cp)
yhat.now = predict(tree.mod)
pred.boost = pred.boost + alpha*predict(tree.mod, newdata=va)
tr.boost = tr.boost + alpha*yhat.now
gbm_tr$y = trtr$y-tr.boost
train_err = mean (sign(tr.boost)!=trtr$y)
test_err = mean (sign(pred.boost)!=va$y)
# cat (i, "train:", train_err, " test:", test_err,"\n")
train_miss_calssification = c(train_miss_calssification,train_err)
test_miss_calssification = c(test_miss_calssification,test_err)
}
out_tbl <- tibble(train_err = train_miss_calssification,
test_err = test_miss_calssification,
iteration = seq(1,n_iter))
return(out_tbl)
}
res3 <- train_val_ada_boost_mse()
res3 %>%
pivot_longer(cols = !iteration,names_to = "err_type",values_to = "err") %>%
ggplot(aes(iteration, err,group=err_type,color = err_type)) +
geom_line(aes(group=err_type)) +
geom_point() +
theme_light()
res4 <- train_val_ada_boost(cp = 0, maxdepth = 3)
########### Training data (rankings only, no dates):
set.seed(123)
con <- url("http://www.tau.ac.il/~saharon/StatsLearn2022/train_ratings_all.dat")
X.tr <- read.table(con)
con <- url("http://www.tau.ac.il/~saharon/StatsLearn2022/train_y_rating.dat")
y.tr <- read.table(con)
con = url("http://www.tau.ac.il/~saharon/StatsLearn2022/movie_titles.txt")
titles = read.table(con,sep=",")
names(X.tr) = substr(as.character(titles[,2]),1,15)
movies = substr(as.character(titles[,2]),1,15)
########### Divide training data into training and validation
n = dim(X.tr)[1]
nva=2000
va.id = sample (n,nva) # choose 2000 points for validation
trtr = data.frame (X = X.tr[-va.id,],y=(y.tr[-va.id,]>3) - (y.tr[-va.id,]<=3))
va = data.frame (X = X.tr[va.id,],y=(y.tr[va.id,]>3)- (y.tr[va.id,]<=3))
############# AdaBoost
train_val_ada_boost <- function(maxdepth = 2, cp = 0.00001, epsilon = 0, n_iter = 1000){
train_miss_calssification <- c()
test_miss_calssification <- c()
w.now = rep (1, dim(trtr)[1]) # initialize w=1
err.boost=err.tr.boost=NULL
pred.boost = numeric(dim(va)[1])
tr.boost = numeric(dim(trtr)[1])
for (i in 1:n_iter){
tree.mod= rpart (y~.,data=trtr,method="class",weights=w.now,maxdepth=maxdepth,cp=cp)
yhat.now = predict(tree.mod,type="class")
yhat.now.num = as.numeric(as.character(yhat.now))
Err = sum( w.now*(yhat.now != trtr$y))/sum(w.now)
if (epsilon == 0){
alpha = 0.5*log((1-Err)/Err)
}
else {alpha = epsilon}
# for sanity check:
# cat(i,Err, alpha,"\n")
pred.boost = pred.boost + alpha*as.numeric(as.character(predict(tree.mod, newdata=va,type="class")))
tr.boost = tr.boost + alpha*yhat.now.num
w.now = w.now * exp(-alpha*yhat.now.num*trtr$y)
w.now = w.now/sum(w.now)*dim(trtr)[1] # rescaling - not important!
train_err = mean (sign(tr.boost)!=trtr$y)
test_err = mean (sign(pred.boost)!=va$y)
# cat (i, "train:", train_err, " test:", test_err,"\n")
train_miss_calssification = c(train_miss_calssification,train_err)
test_miss_calssification = c(test_miss_calssification,test_err)
}
out_tbl <- tibble(train_err = train_miss_calssification,
test_err = test_miss_calssification,
iteration = seq(1,n_iter))
return(out_tbl)
}
train_val_ada_boost_mse <- function(maxdepth = 2, cp = 0.00001, epsilon = 0.01, n_iter = 1000){
gbm_tr = trtr
train_miss_calssification <- c()
test_miss_calssification <- c()
w.now = rep (1, dim(trtr)[1]) # initialize w=1
err.boost=err.tr.boost=NULL
pred.boost = numeric(dim(va)[1])
tr.boost = numeric(dim(trtr)[1])
alpha = epsilon
for (i in 1:n_iter){
tree.mod= rpart (y~.,data=gbm_tr,method="anova",maxdepth=maxdepth,cp=cp)
yhat.now = predict(tree.mod)
pred.boost = pred.boost + alpha*predict(tree.mod, newdata=va)
tr.boost = tr.boost + alpha*yhat.now
gbm_tr$y = trtr$y-tr.boost
train_err = mean (sign(tr.boost)!=trtr$y)
test_err = mean (sign(pred.boost)!=va$y)
# cat (i, "train:", train_err, " test:", test_err,"\n")
train_miss_calssification = c(train_miss_calssification,train_err)
test_miss_calssification = c(test_miss_calssification,test_err)
}
out_tbl <- tibble(train_err = train_miss_calssification,
test_err = test_miss_calssification,
iteration = seq(1,n_iter))
return(out_tbl)
}
res3 <- train_val_ada_boost_mse()
res3 %>%
pivot_longer(cols = !iteration,names_to = "err_type",values_to = "err") %>%
ggplot(aes(iteration, err,group=err_type,color = err_type)) +
geom_line(aes(group=err_type)) +
geom_point() +
theme_light()
train_val_ada_boost_mse <- function(maxdepth = 2, cp = 0.00001, epsilon = 0.01, n_iter = 1000){
gbm_tr = trtr
train_miss_calssification <- c()
test_miss_calssification <- c()
w.now = rep (1, dim(trtr)[1]) # initialize w=1
err.boost=err.tr.boost=NULL
pred.boost = numeric(dim(va)[1])
tr.boost = numeric(dim(trtr)[1])
alpha = epsilon
for (i in 1:n_iter){
tree.mod= rpart (y~.,data=gbm_tr,method="anova",maxdepth=maxdepth,cp=cp)
yhat.now = predict(tree.mod)
pred.boost = pred.boost + alpha*predict(tree.mod, newdata=va)
tr.boost = tr.boost + alpha*yhat.now
gbm_tr$y = trtr$y-tr.boost
train_err = mean (sign(tr.boost)!=trtr$y)
test_err = mean (sign(pred.boost)!=va$y)
# cat (i, "train:", train_err, " test:", test_err,"\n")
train_miss_calssification = c(train_miss_calssification,train_err)
test_miss_calssification = c(test_miss_calssification,test_err)
}
out_tbl <- tibble(train_err = train_miss_calssification,
test_err = test_miss_calssification,
iteration = seq(1,n_iter))
return(out_tbl)
}
res3 <- train_val_ada_boost_mse()
res3 %>%
pivot_longer(cols = !iteration,names_to = "err_type",values_to = "err") %>%
ggplot(aes(iteration, err,group=err_type,color = err_type)) +
geom_line(aes(group=err_type)) +
geom_point() +
theme_light()
library(dplyr)
library(ggplot2)
library(purrr)
library(caret)
library(class)
library(tidymodels)
library(furrr)
library(rpart)
library(rpart)
library(keras)
library(dplyr)
library(ggplot2)
library(purrr)
library(caret)
library(class)
library(tidymodels)
library(furrr)
library(rpart)
library(rpart)
library(keras)
